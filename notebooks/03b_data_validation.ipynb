{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03b - Validazione e Pulizia Dataset\n",
    "\n",
    "Questo notebook gestisce la sincronizzazione tra i CSV e le immagini effettivamente presenti dopo la revisione manuale.\n",
    "\n",
    "**Operazioni eseguite:**\n",
    "1. Scansione delle cartelle con immagini preprocessate\n",
    "2. Confronto con i CSV esistenti\n",
    "3. Rimozione dai CSV delle righe relative a immagini eliminate\n",
    "4. Backup dei CSV originali\n",
    "5. Report delle immagini rimosse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import e Configurazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "# Manipolazione dati\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configurazione\n",
    "import yaml\n",
    "\n",
    "print(f\"Esecuzione: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per trovare la root del progetto\n",
    "def get_project_root():\n",
    "    \"\"\"Rileva la root del progetto.\"\"\"\n",
    "    notebook_dir = Path.cwd()\n",
    "    if notebook_dir.name == 'notebooks':\n",
    "        return notebook_dir.parent\n",
    "    current = notebook_dir\n",
    "    for _ in range(5):\n",
    "        if (current / 'Data').exists():\n",
    "            return current\n",
    "        current = current.parent\n",
    "    # Fallback per WSL\n",
    "    if sys.platform == 'linux' and Path('/mnt/c').exists():\n",
    "        return Path('/mnt/c/Repository/multi-method-xai-diabetic-retinopathy')\n",
    "    return Path('C:/Repository/multi-method-xai-diabetic-retinopathy')\n",
    "\n",
    "PROJECT_ROOT = get_project_root()\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento configurazione\n",
    "config_path = PROJECT_ROOT / 'config.yaml'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Setup percorsi\n",
    "PROCESSED_DIR = PROJECT_ROOT / config['paths']['processed_dir']\n",
    "PREPROCESSED_DIR = PROJECT_ROOT / config['paths']['preprocessed_images']\n",
    "BACKUP_DIR = PROCESSED_DIR / 'backup_csv'\n",
    "\n",
    "print(f\"Directory processed: {PROCESSED_DIR}\")\n",
    "print(f\"Directory immagini: {PREPROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scansione Immagini Esistenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_existing_images(images_dir):\n",
    "    \"\"\"\n",
    "    Scansiona la directory delle immagini preprocessate e restituisce\n",
    "    un set con tutti i nomi dei file esistenti.\n",
    "    \n",
    "    Args:\n",
    "        images_dir: Path alla directory delle immagini\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dizionario {nome_file: percorso_completo}\n",
    "    \"\"\"\n",
    "    existing_images = {}\n",
    "    extensions = {'.png', '.jpg', '.jpeg'}\n",
    "    \n",
    "    for root, dirs, files in os.walk(images_dir):\n",
    "        for file in files:\n",
    "            if Path(file).suffix.lower() in extensions:\n",
    "                full_path = Path(root) / file\n",
    "                # Salva con percorso relativo dalla root del progetto\n",
    "                rel_path = full_path.relative_to(PROJECT_ROOT)\n",
    "                existing_images[file] = str(rel_path)\n",
    "    \n",
    "    return existing_images\n",
    "\n",
    "# Scansiona le immagini\n",
    "print(\"Scansione immagini in corso...\")\n",
    "existing_images = scan_existing_images(PREPROCESSED_DIR)\n",
    "print(f\"Immagini trovate: {len(existing_images):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra alcuni esempi di immagini trovate\n",
    "print(\"\\nEsempi di immagini trovate:\")\n",
    "for i, (name, path) in enumerate(list(existing_images.items())[:5]):\n",
    "    print(f\"  {name} -> {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validazione CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_csv(csv_path, existing_images, project_root):\n",
    "    \"\"\"\n",
    "    Valida un CSV confrontandolo con le immagini esistenti.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path al file CSV\n",
    "        existing_images: Dict delle immagini esistenti\n",
    "        project_root: Root del progetto\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (df_valid, df_removed, stats)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Estrai il nome del file dalla colonna preprocessed_path\n",
    "    def get_filename(path):\n",
    "        return Path(path).name\n",
    "    \n",
    "    df['_filename'] = df['preprocessed_path'].apply(get_filename)\n",
    "    \n",
    "    # Verifica quali immagini esistono\n",
    "    df['_exists'] = df['_filename'].apply(lambda x: x in existing_images)\n",
    "    \n",
    "    # Separa righe valide e rimosse\n",
    "    df_valid = df[df['_exists']].copy()\n",
    "    df_removed = df[~df['_exists']].copy()\n",
    "    \n",
    "    # Aggiorna i percorsi con quelli corretti (nel caso siano cambiati)\n",
    "    df_valid['preprocessed_path'] = df_valid['_filename'].apply(\n",
    "        lambda x: existing_images.get(x, '')\n",
    "    )\n",
    "    \n",
    "    # Rimuovi colonne temporanee\n",
    "    df_valid = df_valid.drop(columns=['_filename', '_exists'])\n",
    "    df_removed = df_removed.drop(columns=['_exists'])\n",
    "    \n",
    "    # Statistiche\n",
    "    stats = {\n",
    "        'original': original_count,\n",
    "        'valid': len(df_valid),\n",
    "        'removed': len(df_removed),\n",
    "        'pct_removed': len(df_removed) / original_count * 100 if original_count > 0 else 0\n",
    "    }\n",
    "    \n",
    "    return df_valid, df_removed, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista dei CSV da validare\n",
    "csv_files = [\n",
    "    'aptos_preprocessed.csv',\n",
    "    'eyepacs_preprocessed.csv',\n",
    "    'messidor2_preprocessed.csv',\n",
    "    'ddr_preprocessed.csv',\n",
    "    'combined_train_preprocessed.csv'\n",
    "]\n",
    "\n",
    "# Verifica quali CSV esistono\n",
    "existing_csvs = [f for f in csv_files if (PROCESSED_DIR / f).exists()]\n",
    "print(f\"CSV trovati: {len(existing_csvs)}\")\n",
    "for csv in existing_csvs:\n",
    "    print(f\"  - {csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Backup e Aggiornamento CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea directory di backup\n",
    "BACKUP_DIR.mkdir(exist_ok=True)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "print(f\"Directory backup: {BACKUP_DIR}\")\n",
    "print(f\"Timestamp: {timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dizionario per raccogliere tutte le statistiche\n",
    "all_stats = {}\n",
    "all_removed = []\n",
    "\n",
    "for csv_file in existing_csvs:\n",
    "    csv_path = PROCESSED_DIR / csv_file\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Elaborazione: {csv_file}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Validazione\n",
    "    df_valid, df_removed, stats = validate_csv(csv_path, existing_images, PROJECT_ROOT)\n",
    "    all_stats[csv_file] = stats\n",
    "    \n",
    "    print(f\"  Righe originali: {stats['original']:,}\")\n",
    "    print(f\"  Righe valide: {stats['valid']:,}\")\n",
    "    print(f\"  Righe rimosse: {stats['removed']:,} ({stats['pct_removed']:.2f}%)\")\n",
    "    \n",
    "    if stats['removed'] > 0:\n",
    "        # Backup del CSV originale\n",
    "        backup_path = BACKUP_DIR / f\"{csv_file.replace('.csv', '')}_{timestamp}.csv\"\n",
    "        shutil.copy(csv_path, backup_path)\n",
    "        print(f\"  Backup salvato: {backup_path.name}\")\n",
    "        \n",
    "        # Salva CSV aggiornato\n",
    "        df_valid.to_csv(csv_path, index=False)\n",
    "        print(f\"  CSV aggiornato salvato\")\n",
    "        \n",
    "        # Aggiungi al report\n",
    "        df_removed['source_csv'] = csv_file\n",
    "        all_removed.append(df_removed)\n",
    "        \n",
    "        # Mostra alcune immagini rimosse\n",
    "        print(f\"\\n  Esempi di immagini rimosse:\")\n",
    "        for _, row in df_removed.head(5).iterrows():\n",
    "            print(f\"    - {row['_filename']}\")\n",
    "    else:\n",
    "        print(f\"  Nessuna modifica necessaria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Report Finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Riepilogo generale\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RIEPILOGO VALIDAZIONE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_original = sum(s['original'] for s in all_stats.values())\n",
    "total_valid = sum(s['valid'] for s in all_stats.values())\n",
    "total_removed = sum(s['removed'] for s in all_stats.values())\n",
    "\n",
    "print(f\"\\nImmagini nel filesystem: {len(existing_images):,}\")\n",
    "print(f\"\\nRighe totali nei CSV:\")\n",
    "print(f\"  Prima: {total_original:,}\")\n",
    "print(f\"  Dopo: {total_valid:,}\")\n",
    "print(f\"  Rimosse: {total_removed:,}\")\n",
    "\n",
    "if total_removed > 0:\n",
    "    print(f\"\\nPercentuale rimossa: {total_removed/total_original*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabella riepilogativa per CSV\n",
    "print(\"\\nDettaglio per CSV:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'CSV':<35} {'Originali':>10} {'Valide':>10} {'Rimosse':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for csv_file, stats in all_stats.items():\n",
    "    print(f\"{csv_file:<35} {stats['original']:>10,} {stats['valid']:>10,} {stats['removed']:>10,}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'TOTALE':<35} {total_original:>10,} {total_valid:>10,} {total_removed:>10,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva report delle immagini rimosse\n",
    "if all_removed:\n",
    "    df_all_removed = pd.concat(all_removed, ignore_index=True)\n",
    "    report_path = PROCESSED_DIR / f'removed_images_report_{timestamp}.csv'\n",
    "    df_all_removed.to_csv(report_path, index=False)\n",
    "    print(f\"\\nReport immagini rimosse salvato: {report_path.name}\")\n",
    "    print(f\"Totale immagini nel report: {len(df_all_removed):,}\")\n",
    "else:\n",
    "    print(\"\\nNessuna immagine rimossa - nessun report generato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuzione classi dopo la pulizia (se ci sono state rimozioni)\n",
    "if total_removed > 0:\n",
    "    print(\"\\nDistribuzione classi dopo la pulizia:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    class_names = config['model']['class_names']\n",
    "    \n",
    "    # Ricarica il combined train aggiornato\n",
    "    combined_path = PROCESSED_DIR / 'combined_train_preprocessed.csv'\n",
    "    if combined_path.exists():\n",
    "        df_train = pd.read_csv(combined_path)\n",
    "        print(f\"\\nTraining set ({len(df_train):,} immagini):\")\n",
    "        \n",
    "        class_counts = df_train['diagnosis'].value_counts().sort_index()\n",
    "        for cls, count in class_counts.items():\n",
    "            pct = count / len(df_train) * 100\n",
    "            print(f\"  Classe {cls} ({class_names[cls]}): {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ricalcolo Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se ci sono state modifiche, ricalcola i class weights\n",
    "if total_removed > 0:\n",
    "    import torch\n",
    "    \n",
    "    combined_path = PROCESSED_DIR / 'combined_train_preprocessed.csv'\n",
    "    if combined_path.exists():\n",
    "        df_train = pd.read_csv(combined_path)\n",
    "        labels = df_train['diagnosis'].values\n",
    "        \n",
    "        # Calcola nuovi class weights\n",
    "        class_counts = np.bincount(labels)\n",
    "        n_samples = len(labels)\n",
    "        n_classes = len(class_counts)\n",
    "        weights = n_samples / (n_classes * class_counts)\n",
    "        class_weights = torch.FloatTensor(weights)\n",
    "        \n",
    "        # Backup dei vecchi weights\n",
    "        old_weights_path = PROCESSED_DIR / 'class_weights.pt'\n",
    "        if old_weights_path.exists():\n",
    "            backup_weights = BACKUP_DIR / f'class_weights_{timestamp}.pt'\n",
    "            shutil.copy(old_weights_path, backup_weights)\n",
    "            print(f\"Backup class weights: {backup_weights.name}\")\n",
    "        \n",
    "        # Salva nuovi weights\n",
    "        torch.save(class_weights, old_weights_path)\n",
    "        \n",
    "        print(\"\\nNuovi class weights calcolati:\")\n",
    "        for i, w in enumerate(class_weights):\n",
    "            print(f\"  Classe {i} ({class_names[i]}): {w:.4f}\")\n",
    "else:\n",
    "    print(\"Nessuna modifica ai CSV - class weights invariati\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusioni\n",
    "\n",
    "### Operazioni Completate\n",
    "\n",
    "1. **Scansione filesystem**: Identificate tutte le immagini preprocessate esistenti\n",
    "2. **Validazione CSV**: Confronto tra riferimenti nei CSV e immagini reali\n",
    "3. **Backup**: Salvati i CSV originali prima delle modifiche\n",
    "4. **Aggiornamento**: Rimossi dai CSV i riferimenti a immagini eliminate\n",
    "5. **Report**: Generato elenco delle immagini rimosse\n",
    "6. **Class Weights**: Ricalcolati i pesi per il training\n",
    "\n",
    "### File Generati\n",
    "\n",
    "- `backup_csv/`: Directory con i CSV originali\n",
    "- `removed_images_report_YYYYMMDD_HHMMSS.csv`: Elenco immagini rimosse\n",
    "- `class_weights.pt`: Pesi aggiornati per il training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
